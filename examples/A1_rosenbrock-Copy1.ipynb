{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "forty-conspiracy",
   "metadata": {},
   "source": [
    "# Rosenbrock\n",
    "\n",
    "Minimize 2-variable nonsmooth Rosenbrock function, subject to a simple bound constraint. Taken from: [GRANSO](http://www.timmitchell.com/software/GRANSO/) demo examples 1, 2, & 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-premises",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-individual",
   "metadata": {},
   "source": [
    "$$\\min_{x_1,x_2} w|x_1^2-x_2|+(1-x_1)^2,$$\n",
    "$$\\text{s.t. }c_1(x_1,x_2) = \\sqrt{2}x_1-1 \\leq 0, c_(x_1,x_2)=2x_2-1\\leq0,$$\n",
    "\n",
    "where $w$ is a constant (e.g., $w=8$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-layout",
   "metadata": {},
   "source": [
    "## Modules Importing\n",
    "Import all necessary modules and add PyGRANSO src folder to system path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "manual-pillow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/5/dever120/PyGRANSO\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "supreme-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from pygranso.pygranso import pygranso\n",
    "from pygranso.pygransoStruct import pygransoStruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-penny",
   "metadata": {},
   "source": [
    "## Function Set-Up\n",
    "\n",
    "Encode the optimization variables, and objective and constraint functions.\n",
    "\n",
    "Note: please strictly follow the format of comb_fn, which will be used in the PyGRANSO main algortihm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "disabled-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "# variables and corresponding dimensions.\n",
    "var_in = {\"x1\": [1], \"x2\": [1]}\n",
    "\n",
    "\n",
    "def comb_fn(X_struct):\n",
    "    x1 = X_struct.x1\n",
    "    x2 = X_struct.x2\n",
    "\n",
    "    # objective function\n",
    "    f = 8 * abs(x1**2 - x2) + (1 - x1) ** 2\n",
    "\n",
    "    # inequality constraint, matrix form\n",
    "    ci = pygransoStruct()\n",
    "    ci.c1 = (2**0.5) * x1 - 1\n",
    "    ci.c2 = 2 * x2 - 1\n",
    "\n",
    "    # equality constraint\n",
    "    ce = None\n",
    "\n",
    "    return [f, ci, ce]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-client",
   "metadata": {},
   "source": [
    "## User Options\n",
    "Specify user-defined options for PyGRANSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "precise-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = pygransoStruct()\n",
    "# option for switching QP solver. We only have osqp as the only qp solver in current version. Default is osqp\n",
    "# opts.QPsolver = 'osqp'\n",
    "\n",
    "# set an intial point\n",
    "# All the user-provided data (vector/matrix/tensor) must be in torch tensor format.\n",
    "# As PyTorch tensor is single precision by default, one must explicitly set `dtype=torch.double`.\n",
    "# Also, please make sure the device of provided torch tensor is the same as opts.torch_device.\n",
    "opts.x0 = torch.ones((2, 1), device=device, dtype=torch.double)\n",
    "opts.torch_device = device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-europe",
   "metadata": {},
   "source": [
    "## Main Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "soln = pygranso(var_spec=var_in, combined_fn=comb_fn, user_opts=opts)\n",
    "end = time.time()\n",
    "print(\"Total Wall Time: {}s\".format(end - start))\n",
    "print(soln.final.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-apartment",
   "metadata": {},
   "source": [
    "## PyGRANSO Restarting\n",
    "**(Optional)** The following example shows how to set various PyGRANSO options (such as simpler ASCII printing) and how to restart PyGRANSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = pygransoStruct()\n",
    "opts.torch_device = device\n",
    "# set an infeasible initial point\n",
    "opts.x0 = 5.5 * torch.ones((2, 1), device=device, dtype=torch.double)\n",
    "\n",
    "# By default PyGRANSO will print using extended ASCII characters to 'draw' table borders and some color prints.\n",
    "# If user wants to create a log txt file of the console output, please set opts.print_ascii = True\n",
    "opts.print_ascii = True\n",
    "\n",
    "# By default, PyGRANSO prints an info message about QP solvers, since\n",
    "# PyGRANSO can be used with any QP solver that has a quadprog-compatible\n",
    "# interface.  Let's disable this message since we've already seen it\n",
    "# hundreds of times and can now recite it from memory.  ;-)\n",
    "opts.quadprog_info_msg = False\n",
    "\n",
    "# Try a very short run.\n",
    "opts.maxit = 10  # default is 1000\n",
    "\n",
    "# PyGRANSO's penalty parameter is on the *objective* function, thus\n",
    "# higher penalty parameter values favor objective minimization more\n",
    "# highly than attaining feasibility.  Let's set PyGRANSO to start off\n",
    "# with a higher initial value of the penalty parameter.  PyGRANSO will\n",
    "# automatically tune the penalty parameter to promote progress towards\n",
    "# feasibility.  PyGRANSO only adjusts the penalty parameter in a\n",
    "# monotonically decreasing fashion.\n",
    "opts.mu0 = 100  # default is 1\n",
    "\n",
    "# start main algorithm\n",
    "soln = pygranso(var_spec=var_in, combined_fn=comb_fn, user_opts=opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-insight",
   "metadata": {},
   "source": [
    "Let's restart PyGRANSO from the last iterate of the previous run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = pygransoStruct()\n",
    "opts.torch_device = device\n",
    "# set the initial point and penalty parameter to their final values from the previous run\n",
    "opts.x0 = soln.final.x\n",
    "opts.mu0 = soln.final.mu\n",
    "opts.opt_tol = 1e-6\n",
    "\n",
    "# PREPARE TO RESTART PyGRANSO IN FULL-MEMORY MODE\n",
    "# Set the last BFGS inverse Hessian approximation as the initial\n",
    "# Hessian for the next run.  Generally this is a good thing to do, and\n",
    "# often it is necessary to retain this information when restarting (as\n",
    "# on difficult nonsmooth problems, PyGRANSO may not be able to restart\n",
    "# without it).  However, your mileage may vary.  In the test, with\n",
    "# the above settings, omitting H0 causes PyGRANSO to take an additional\n",
    "# 16 iterations to converge on this problem.\n",
    "opts.H0 = soln.H_final  # try running with this commented out\n",
    "\n",
    "# When restarting, soln.H_final may fail PyGRANSO's initial check to\n",
    "# assess whether or not the user-provided H0 is positive definite.  If\n",
    "# it fails this test, the test may be disabled by setting opts.checkH0\n",
    "# to false.\n",
    "# opts.checkH0 = False       % Not needed for this example\n",
    "\n",
    "# If one desires to restart PyGRANSO as if it had never stopped (e.g.\n",
    "# to continue optimization after it hit its maxit limit), then one must\n",
    "# also disable scaling the initial BFGS inverse Hessian approximation\n",
    "# on the very first iterate.\n",
    "opts.scaleH0 = False\n",
    "\n",
    "# Restart PyGRANSO\n",
    "opts.maxit = 100  # increase maximum allowed iterations\n",
    "\n",
    "# Main algorithm\n",
    "soln = pygranso(var_spec=var_in, combined_fn=comb_fn, user_opts=opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "soln.final.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-tooth",
   "metadata": {},
   "source": [
    "## Results Logs\n",
    "\n",
    "**(Optional)** opts below shows the importance of using an initial point that is neither near\n",
    "nor on a nonsmooth manifold, that is, the functions \n",
    "(objective and constraints) should be smooth at and *about* \n",
    "the initial point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = pygransoStruct()\n",
    "opts.torch_device = device\n",
    "# Set a randomly generated starting point.  In theory, with probability\n",
    "# one, a randomly selected point will not be on a nonsmooth manifold.\n",
    "opts.x0 = torch.randn(\n",
    "    (2, 1), device=device, dtype=torch.double\n",
    ")  # randomly generated is okay\n",
    "opts.maxit = 100  # we'll use this value of maxit later\n",
    "opts.opt_tol = 1e-6\n",
    "\n",
    "# However, (0,0) or (1,1) are on the nonsmooth manifold and if PyGRANSO\n",
    "# is started at either of them, it will break down on the first\n",
    "# iteration.  This example highlights that it is imperative to start\n",
    "# PyGRANSO at a point where the functions are smooth.\n",
    "\n",
    "# Uncomment either of the following two lines to try starting PyGRANSO\n",
    "# from (0,0) or (1,1), where the functions are not differentiable.\n",
    "\n",
    "# opts.x0 = torch.ones((2,1), device=device, dtype=torch.double)     # uncomment this line to try this point\n",
    "# opts.x0 = torch.zeros((2,1), device=device, dtype=torch.double)    # uncomment this line to try this point\n",
    "\n",
    "# Uncomment the following two lines to try starting PyGRANSO from a\n",
    "# uniformly perturbed version of (1,1).  pert_level needs to be at\n",
    "# least 1e-3 or so to get consistently reliable optimization quality.\n",
    "\n",
    "# pert_level = 1e-3\n",
    "# opts.x0 = (torch.ones((2,1)) + pert_level * (torch.randn((2,1)) - 0.5)).to(device=device, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-speech",
   "metadata": {},
   "source": [
    "The opts below show how to use opts.halt_log_fn to create a history of iterates\n",
    "\n",
    "NOTE: NO NEED TO CHANGE ANYTHING BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP THE LOGGING FEATURES\n",
    "\n",
    "# Set up PyGRANSO's logging functions; pass opts.maxit to it so that\n",
    "# storage can be preallocated for efficiency.\n",
    "\n",
    "\n",
    "class HaltLog:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def haltLog(\n",
    "        self,\n",
    "        iteration,\n",
    "        x,\n",
    "        penaltyfn_parts,\n",
    "        d,\n",
    "        get_BFGS_state_fn,\n",
    "        H_regularized,\n",
    "        ls_evals,\n",
    "        alpha,\n",
    "        n_gradients,\n",
    "        stat_vec,\n",
    "        stat_val,\n",
    "        fallback_level,\n",
    "    ):\n",
    "        # DON'T CHANGE THIS\n",
    "        # increment the index/count\n",
    "        self.index += 1\n",
    "\n",
    "        # EXAMPLE:\n",
    "        # store history of x iterates in a preallocated cell array\n",
    "        self.x_iterates.append(x)\n",
    "        self.f.append(penaltyfn_parts.f)\n",
    "        self.tv.append(penaltyfn_parts.tv)\n",
    "\n",
    "        # keep this false unless you want to implement a custom termination\n",
    "        # condition\n",
    "        halt = False\n",
    "        return halt\n",
    "\n",
    "    # Once PyGRANSO has run, you may call this function to get retreive all\n",
    "    # the logging data stored in the shared variables, which is populated\n",
    "    # by haltLog being called on every iteration of PyGRANSO.\n",
    "    def getLog(self):\n",
    "        # EXAMPLE\n",
    "        # return x_iterates, trimmed to correct size\n",
    "        log = pygransoStruct()\n",
    "        log.x = self.x_iterates[0 : self.index]\n",
    "        log.f = self.f[0 : self.index]\n",
    "        log.tv = self.tv[0 : self.index]\n",
    "        return log\n",
    "\n",
    "    def makeHaltLogFunctions(self, maxit):\n",
    "        # don't change these lambda functions\n",
    "        halt_log_fn = (\n",
    "            lambda iteration,\n",
    "            x,\n",
    "            penaltyfn_parts,\n",
    "            d,\n",
    "            get_BFGS_state_fn,\n",
    "            H_regularized,\n",
    "            ls_evals,\n",
    "            alpha,\n",
    "            n_gradients,\n",
    "            stat_vec,\n",
    "            stat_val,\n",
    "            fallback_level: self.haltLog(\n",
    "                iteration,\n",
    "                x,\n",
    "                penaltyfn_parts,\n",
    "                d,\n",
    "                get_BFGS_state_fn,\n",
    "                H_regularized,\n",
    "                ls_evals,\n",
    "                alpha,\n",
    "                n_gradients,\n",
    "                stat_vec,\n",
    "                stat_val,\n",
    "                fallback_level,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        get_log_fn = lambda: self.getLog()\n",
    "\n",
    "        # Make your shared variables here to store PyGRANSO history data\n",
    "        # EXAMPLE - store history of iterates x_0,x_1,...,x_k\n",
    "        self.index = 0\n",
    "        self.x_iterates = []\n",
    "        self.f = []\n",
    "        self.tv = []\n",
    "\n",
    "        # Only modify the body of logIterate(), not its name or arguments.\n",
    "        # Store whatever data you wish from the current PyGRANSO iteration info,\n",
    "        # given by the input arguments, into shared variables of\n",
    "        # makeHaltLogFunctions, so that this data can be retrieved after PyGRANSO\n",
    "        # has been terminated.\n",
    "        #\n",
    "        # DESCRIPTION OF INPUT ARGUMENTS\n",
    "        #   iter                current iteration number\n",
    "        #   x                   current iterate x\n",
    "        #   penaltyfn_parts     struct containing the following\n",
    "        #       OBJECTIVE AND CONSTRAINTS VALUES\n",
    "        #       .f              objective value at x\n",
    "        #       .f_grad         objective gradient at x\n",
    "        #       .ci             inequality constraint at x\n",
    "        #       .ci_grad        inequality gradient at x\n",
    "        #       .ce             equality constraint at x\n",
    "        #       .ce_grad        equality gradient at x\n",
    "        #       TOTAL VIOLATION VALUES (inf norm, for determining feasibiliy)\n",
    "        #       .tvi            total violation of inequality constraints at x\n",
    "        #       .tve            total violation of equality constraints at x\n",
    "        #       .tv             total violation of all constraints at x\n",
    "        #       TOTAL VIOLATION VALUES (one norm, for L1 penalty function)\n",
    "        #       .tvi_l1         total violation of inequality constraints at x\n",
    "        #       .tvi_l1_grad    its gradient\n",
    "        #       .tve_l1         total violation of equality constraints at x\n",
    "        #       .tve_l1_grad    its gradient\n",
    "        #       .tv_l1          total violation of all constraints at x\n",
    "        #       .tv_l1_grad     its gradient\n",
    "        #       PENALTY FUNCTION VALUES\n",
    "        #       .p              penalty function value at x\n",
    "        #       .p_grad         penalty function gradient at x\n",
    "        #       .mu             current value of the penalty parameter\n",
    "        #       .feasible_to_tol logical indicating whether x is feasible\n",
    "        #   d                   search direction\n",
    "        #   get_BFGS_state_fn   function handle to get the (L)BFGS state data\n",
    "        #                       FULL MEMORY:\n",
    "        #                       - returns BFGS inverse Hessian approximation\n",
    "        #                       LIMITED MEMORY:\n",
    "        #                       - returns a struct with current L-BFGS state:\n",
    "        #                           .S          matrix of the BFGS s vectors\n",
    "        #                           .Y          matrix of the BFGS y vectors\n",
    "        #                           .rho        row vector of the 1/sty values\n",
    "        #                           .gamma      H0 scaling factor\n",
    "        #   H_regularized       regularized version of H\n",
    "        #                       [] if no regularization was applied to H\n",
    "        #   fn_evals            number of function evaluations incurred during\n",
    "        #                       this iteration\n",
    "        #   alpha               size of accepted size\n",
    "        #   n_gradients         number of previous gradients used for computing\n",
    "        #                       the termination QP\n",
    "        #   stat_vec            stationarity measure vector\n",
    "        #   stat_val            approximate value of stationarity:\n",
    "        #                           norm(stat_vec)\n",
    "        #                       gradients (result of termination QP)\n",
    "        #   fallback_level      number of strategy needed for a successful step\n",
    "        #                       to be taken.  See bfgssqpOptionsAdvanced.\n",
    "        #\n",
    "        # OUTPUT ARGUMENT\n",
    "        #   halt                set this to true if you wish optimization to\n",
    "        #                       be halted at the current iterate.  This can be\n",
    "        #                       used to create a custom termination condition,\n",
    "        return [halt_log_fn, get_log_fn]\n",
    "\n",
    "\n",
    "mHLF_obj = HaltLog()\n",
    "[halt_log_fn, get_log_fn] = mHLF_obj.makeHaltLogFunctions(opts.maxit)\n",
    "\n",
    "#  Set PyGRANSO's logging function in opts\n",
    "opts.halt_log_fn = halt_log_fn\n",
    "\n",
    "# Main algorithm with logging enabled.\n",
    "soln = pygranso(var_spec=var_in, combined_fn=comb_fn, user_opts=opts)\n",
    "\n",
    "# GET THE HISTORY OF ITERATES\n",
    "# Even if an error is thrown, the log generated until the error can be\n",
    "# obtained by calling get_log_fn()\n",
    "log = get_log_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log.f[0:3])\n",
    "print(log.x[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-train",
   "metadata": {},
   "source": [
    "## LFBGS Restarting\n",
    " \n",
    "**(Optional)**\n",
    "\n",
    " (Note that this example problem only has two variables!)\n",
    " \n",
    " If PyGRANSO runs in limited-memory mode, that is, if \n",
    " opts.limited_mem_size > 0, then PyGRANSO's restart procedure is \n",
    " slightly different from the BFGS restarting, as soln.H_final will instead contain the most \n",
    " current L-BFGS state, not a full inverse Hessian approximation.  \n",
    " \n",
    " Instead the BFGS standard procedure, users should do the following: \n",
    " 1) If you set a specific H0, you will need to set opts.H0 to whatever\n",
    "    you used previously.  By default, PyGRANSO uses the identity for H0.\n",
    "    \n",
    " 2) Warm-start PyGRANSO with the most recent L-BFGS data by setting:\n",
    "    opts.limited_mem_warm_start = soln.H_final;\n",
    "    \n",
    " NOTE: how to set opts.scaleH0 so that PyGRANSO will be restarted as if\n",
    " it had never terminated depends on the previously used values of \n",
    " opts.scaleH0 and opts.limited_mem_fixed_scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = pygransoStruct()\n",
    "opts.torch_device = device\n",
    "# set an infeasible initial point\n",
    "opts.x0 = 5.5 * torch.ones((2, 1), device=device, dtype=torch.double)\n",
    "\n",
    "opts.print_ascii = True\n",
    "opts.quadprog_info_msg = False\n",
    "opts.maxit = 10  # default is 1000\n",
    "opts.mu0 = 100  # default is 1\n",
    "opts.print_frequency = 2\n",
    "\n",
    "\n",
    "# By default, PyGRANSO uses full-memory BFGS updating.  For nonsmooth\n",
    "# problems, full-memory BFGS is generally recommended.  However, if\n",
    "# this is not feasible, one may optionally enable limited-memory BFGS\n",
    "# updating by setting opts.limited_mem_size to a positive integer\n",
    "# (significantly) less than the number of variables.\n",
    "opts.limited_mem_size = 1\n",
    "\n",
    "# start main algorithm\n",
    "soln = pygranso(var_spec=var_in, combined_fn=comb_fn, user_opts=opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart\n",
    "opts = pygransoStruct()\n",
    "opts.torch_device = device\n",
    "# set the initial point and penalty parameter to their final values from the previous run\n",
    "opts.x0 = soln.final.x\n",
    "opts.mu0 = soln.final.mu\n",
    "opts.limited_mem_size = 1\n",
    "opts.quadprog_info_msg = False\n",
    "opts.print_frequency = 2\n",
    "\n",
    "opts.limited_mem_warm_start = soln.H_final\n",
    "opts.scaleH0 = False\n",
    "\n",
    "# In contrast to full-memory BFGS updating, limited-memory BFGS\n",
    "# permits that H0 can be scaled on every iteration.  By default,\n",
    "# PyGRANSO will reuse the scaling parameter that is calculated on the\n",
    "# very first iteration for all subsequent iterations as well.  Set\n",
    "# this option to false to force PyGRANSO to calculate a new scaling\n",
    "# parameter on every iteration.  Note that opts.scaleH0 has no effect\n",
    "# when opts.limited_mem_fixed_scaling is set to true.\n",
    "opts.limited_mem_fixed_scaling = False\n",
    "\n",
    "# Restart PyGRANSO\n",
    "opts.maxit = 100  # increase maximum allowed iterations\n",
    "\n",
    "# Main algorithm\n",
    "soln = pygranso(var_spec=var_in, combined_fn=comb_fn, user_opts=opts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimization",
   "language": "python",
   "name": "optimization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

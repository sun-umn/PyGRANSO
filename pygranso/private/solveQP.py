import sys
import traceback

import gurobipy as gp
import numpy as np
import osqp
import torch
from gurobipy import GRB
from scipy import sparse

QP_REQUESTS = 0


def solveQP(
    H,
    f,
    A,
    b,
    LB,
    UB,
    QPsolver,
    torch_device,
    double_precision,
    cuda_osqp_enabled=True,
    source="Unknown",
):
    """
    solveQP:
        Convenience wrapper for any quadprog interface QP solver.  This
        wrapper function will suppress any warnings from the underlying
        quadprog solver and catch any errors that are thrown and rethrow
        them as specific error types, such that they can be caught and
        dealt with accordingly.

        solveQP also keeps tracks of the number of times:
        - solveQP has called quadprog
        - quadprog has thrown an error or returned an invalid result;
            a result is considered invalid if it is exactly zero, contains
            infs/NaNs, or is empty.

        To access this metadata:
        [requests,errors] = solveQP('counts');

        To reset these counters:
        clear solveQP;

        INPUT:
        same arguments as quadprog plus an additional field to
        quadprog's options struct:

        .suppress_warnings                  [logical | {true}]
            suppress all warnings and error messages generated by
            quadprog (except for license issues)

        OUTPUT:
        X and LAMBDA output arguments from full call to quadprog:
        [X,FVAL,EXITFLAG,OUTPUT,LAMBDA] = quadprog(...)

        If you publish work that uses or refers to PyGRANSO, please cite both
        PyGRANSO and GRANSO paper:

        [1] Buyun Liang, Tim Mitchell, and Ju Sun,
            NCVX: A User-Friendly and Scalable Package for Nonconvex
            Optimization in Machine Learning, arXiv preprint arXiv:2111.13984 (2021).
            Available at https://arxiv.org/abs/2111.13984

        [2] Frank E. Curtis, Tim Mitchell, and Michael L. Overton,
            A BFGS-SQP method for nonsmooth, nonconvex, constrained
            optimization and its evaluation using relative minimization
            profiles, Optimization Methods and Software, 32(1):148-181, 2017.
            Available at https://dx.doi.org/10.1080/10556788.2016.1208749

        solveQP.py (introduced in PyGRANSO v1.0.0)
        Copyright (C) 2016-2021 Tim Mitchell and Buyun Liang

        This file is a MATLAB-to-Python port of solveQP.m from
        GRANSO v1.6.4 with the following new functionality and/or changes:
            1. Replace MATLAB quadprog solver with OSQP https://osqp.org/
        Ported from MATLAB to Python and modified by Buyun Liang, 2021

        For comments/bug reports, please visit the PyGRANSO webpage:
        https://github.com/sun-umn/PyGRANSO

        =========================================================================
        |  PyGRANSO: A PyTorch-enabled port of GRANSO with auto-differentiation |
        |  Copyright (C) 2021 Tim Mitchell and Buyun Liang                      |
        |                                                                       |
        |  This file is part of PyGRANSO.                                       |
        |                                                                       |
        |  PyGRANSO is free software: you can redistribute it and/or modify     |
        |  it under the terms of the GNU Affero General Public License as       |
        |  published by the Free Software Foundation, either version 3 of       |
        |  the License, or (at your option) any later version.                  |
        |                                                                       |
        |  PyGRANSO is distributed in the hope that it will be useful,          |
        |  but WITHOUT ANY WARRANTY; without even the implied warranty of       |
        |  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the        |
        |  GNU Affero General Public License for more details.                  |
        |                                                                       |
        |  You should have received a copy of the GNU Affero General Public     |
        |  License along with this program.  If not, see                        |
        |  <http://www.gnu.org/licenses/agpl.html>.                             |
        =========================================================================
    """
    global QP_REQUESTS
    QP_REQUESTS += 1

    # ========================================================================
    # DEBUG: Print QP problem information
    # ========================================================================
    print(f"\n{'=' * 80}")
    print(f"QP SOLVER CALL - Source: {source}")
    print(f"{'=' * 80}")
    print(f"H shape: {H.shape if hasattr(H, 'shape') else 'N/A'}")
    print(f"f shape: {f.shape if hasattr(f, 'shape') else 'N/A'}")
    if A is not None:
        print(f"A shape: {A.shape if hasattr(A, 'shape') else 'N/A'}")
    else:
        print(f"A: None (no equality constraints)")
    if b is not None:
        print(f"b shape: {b.shape if hasattr(b, 'shape') else type(b)}")
    else:
        print(f"b: None")
    print(f"LB shape: {LB.shape if hasattr(LB, 'shape') else 'N/A'}")
    print(f"UB shape: {UB.shape if hasattr(UB, 'shape') else 'N/A'}")
    print(f"QPsolver: {QPsolver}")
    print(f"{'=' * 80}\n")

    if QPsolver == "osqp":
        # H,f always exist
        nvar = len(f)

        # OPTIMIZATION: Use pinned memory for faster CPU-GPU transfers
        # Only allocate pinned memory if on CUDA device
        if torch_device.type == "cuda":
            # Transfer to CPU first, then pin for faster transfers
            # Note: Must do .cpu() BEFORE .pin_memory()
            H_cpu = H.cpu().pin_memory().numpy()
            f_cpu = f.cpu().pin_memory().numpy()
            LB_cpu = LB.cpu().pin_memory().numpy()
            UB_cpu = UB.cpu().pin_memory().numpy()

            if A is not None:
                A_cpu = A.cpu().pin_memory().numpy()
            else:
                A_cpu = None
        else:
            # Already on CPU, just convert
            H_cpu = H.numpy()
            f_cpu = f.numpy()
            LB_cpu = LB.numpy()
            UB_cpu = UB.numpy()
            A_cpu = A.numpy() if A is not None else None

        # Convert to sparse format (CSC is most efficient for OSQP)
        H_sparse = sparse.csc_matrix(H_cpu)

        # Setup constraints
        if np.any(A_cpu is not None) and np.any(b is not None):
            Aeq = A_cpu
            beq = b
            speye = sparse.eye(nvar, format="csc")  # Directly create CSC format
            LB_new = np.vstack((beq, LB_cpu))
            UB_new = np.vstack((beq, UB_cpu))
            # Use sparse.vstack directly to CSC
            A_new = sparse.vstack([Aeq, speye], format="csc")
        else:
            #  no constraint A*x == b
            A_new = sparse.eye(nvar, format="csc")  # Direct CSC creation
            LB_new = LB_cpu
            UB_new = UB_cpu

        # Create an OSQP object
        # For CPU QP (when cuda version doesn't work well), use builtin
        if cuda_osqp_enabled:
            algebra_type = "cuda"
        else:
            algebra_type = "builtin"

        prob = osqp.OSQP(algebra=algebra_type)

        # Setup workspace and change alpha parameter
        prob.setup(
            H_sparse,
            f_cpu,
            A_new,
            LB_new,
            UB_new,
            eps_abs=1e-12,
            eps_rel=1e-12,
            polish=True,
            verbose=False,
        )

        # Solve problem
        res = prob.solve()

        # OPTIMIZATION: Convert back to GPU efficiently
        solution = res.x.reshape(-1, 1)  # Reshape in NumPy (faster)

        if double_precision:
            torch_dtype = torch.double
        else:
            torch_dtype = torch.float

        # Direct conversion with pinned memory if on CUDA
        if torch_device.type == "cuda":
            # Create tensor on CPU with pinned memory, transfer to GPU
            solution_tensor = (
                torch.from_numpy(solution)
                .pin_memory()
                .to(device=torch_device, dtype=torch_dtype, non_blocking=True)
            )
        else:
            solution_tensor = torch.from_numpy(solution).to(
                device=torch_device, dtype=torch_dtype
            )
        return solution_tensor

    if QPsolver == "gurobi":
        # OPTIMIZATION: Use pinned memory for faster CPU-GPU transfers
        if torch_device.type == "cuda":
            # Transfer to CPU first, then pin for faster transfers
            H_cpu = H.cpu().pin_memory().numpy()
            f_cpu = f.cpu().pin_memory().numpy()
            LB_cpu = LB.cpu().pin_memory().numpy()
            UB_cpu = UB.cpu().pin_memory().numpy()

            if A is not None:
                A_cpu = A.cpu().pin_memory().numpy()
            else:
                A_cpu = None
        else:
            # Already on CPU
            H_cpu = H.numpy()
            f_cpu = f.numpy()
            LB_cpu = LB.numpy()
            UB_cpu = UB.numpy()
            A_cpu = A.numpy() if A is not None else None

        # H,f always exist
        # LB and UB always exist
        #  formulation of QP has no 1/2
        H_cpu = H_cpu / 2

        nvar = len(f_cpu)
        # Create a new model
        m = gp.Model()
        vtype = [GRB.CONTINUOUS] * nvar

        # Add variables to model
        vars = []
        for j in range(nvar):
            vars.append(m.addVar(lb=LB_cpu[j], ub=UB_cpu[j], vtype=vtype[j]))
        x_vec = np.array(vars).reshape(nvar, 1)

        if np.any(A_cpu is not None) and np.any(b is not None):
            Aeq = A_cpu
            beq = b
            # Populate A matrix
            expr = gp.LinExpr()
            Ax = Aeq @ x_vec
            expr += Ax[0, 0]
            m.addLConstr(expr, GRB.GREATER_EQUAL, beq)
        else:
            #  no constraint A*x < b
            pass

        # Pre-allocate solution array
        dtype = np.float64 if double_precision else np.float32
        solution = np.zeros((nvar, 1), dtype=dtype)

        # Populate objective: x.THx + f.T x
        obj = gp.QuadExpr()
        xTHx = x_vec.T @ H_cpu @ x_vec + f_cpu.T @ x_vec
        obj += xTHx[0, 0]
        m.setObjective(obj)

        #  suppress output
        # m.Params.LogToConsole = 0
        m.Params.outputflag = 0
        # m.params.NonConvex = 2

        m.optimize()
        x = m.getAttr("x", vars)

        # Vectorized assignment instead of loop
        solution[:, 0] = x

        if double_precision:
            torch_dtype = torch.double
        else:
            torch_dtype = torch.float

        # OPTIMIZATION: Efficient GPU transfer with pinned memory
        if torch_device.type == "cuda":
            solution_tensor = (
                torch.from_numpy(solution)
                .pin_memory()
                .to(device=torch_device, dtype=torch_dtype, non_blocking=True)
            )
        else:
            solution_tensor = torch.from_numpy(solution).to(
                device=torch_device, dtype=torch_dtype
            )

        return solution_tensor


def getErr():
    # getErr NOT used
    global QP_REQUESTS
    errors = 0
    return [QP_REQUESTS, errors]
